{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>TP 2</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploration de l'environnement FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'état: Discrete(16)\n",
      "Espace d'action: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v1',is_slippery=True)\n",
    "\n",
    "# Affichage des informations de l'espace d'état et d'action\n",
    "print(f\"Espace d'état: {env.observation_space}\")  # Nombre d'états\n",
    "print(f\"Espace d'action: {env.action_space}\")    # Nombre d'actions possibles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imlpementation de la Q-Table et Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table avant l'apprentissage :\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "État 0: [0. 0. 0. 0.]\n",
      "État 1: [0. 0. 0. 0.]\n",
      "État 2: [0. 0. 0. 0.]\n",
      "État 3: [0. 0. 0. 0.]\n",
      "État 4: [0. 0. 0. 0.]\n",
      "État 5: [0. 0. 0. 0.]\n",
      "État 6: [0. 0. 0. 0.]\n",
      "État 7: [0. 0. 0. 0.]\n",
      "État 8: [0. 0. 0. 0.]\n",
      "État 9: [0. 0. 0. 0.]\n",
      "État 10: [0. 0. 0. 0.]\n",
      "État 11: [0. 0. 0. 0.]\n",
      "État 12: [0. 0. 0. 0.]\n",
      "État 13: [0. 0. 0. 0.]\n",
      "État 14: [0. 0. 0. 0.]\n",
      "État 15: [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Récupérer le nombre d'états et d'actions\n",
    "nb_etats = env.observation_space.n\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Création de la Q-table initialisée à zéro\n",
    "Q_table = np.zeros((nb_etats, nb_actions))\n",
    "\n",
    "# Affichage de la Q-table avant apprentissage\n",
    "print(\"Q-table avant l'apprentissage :\")\n",
    "print(Q_table)\n",
    "\n",
    "# Vérification que chaque état a une liste de valeurs associées aux actions possibles\n",
    "for etat in range(nb_etats):\n",
    "    print(f\"État {etat}: {Q_table[etat]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementation du Q-Learning avec Mise a jour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.00511548e-01 4.14119636e-01 3.76693284e-01 3.78210308e-01]\n",
      " [1.77531987e-03 2.46640496e-01 2.44252704e-05 4.70902233e-02]\n",
      " [2.97856491e-01 3.82499183e-03 2.98266617e-02 2.17263079e-03]\n",
      " [4.93375594e-05 2.01334495e-05 0.00000000e+00 0.00000000e+00]\n",
      " [5.19151020e-01 3.78449133e-01 3.09760910e-01 3.02645706e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.52072562e-02 4.58775896e-02 2.36401788e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.12541366e-01 3.47521604e-01 3.42891612e-01 5.48434568e-01]\n",
      " [3.30037817e-01 5.82593893e-01 4.00582797e-01 1.97671396e-01]\n",
      " [5.72796838e-01 8.32165307e-02 2.68754571e-01 5.05302320e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.17494349e-01 3.07277205e-01 6.98458190e-01 2.61115799e-01]\n",
      " [3.82545567e-01 8.93329212e-01 4.61369789e-01 4.71516230e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Parametres\n",
    "alpha= 0.1\n",
    "gamma=0.99\n",
    "epsilon=1.0\n",
    "epsilon_decay=0.995\n",
    "num_episodes=5000\n",
    "\n",
    "#Boucle d'apprentissage\n",
    "for _ in range(num_episodes):\n",
    "     state, _ = env.reset()\n",
    "     done = False\n",
    "     while not done:\n",
    "          if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()     #Exploration\n",
    "          else:\n",
    "            action = np.argmax(Q_table[state, :])  #Exploitation\n",
    "          \n",
    "          # Exécuter l'action et obtenir la récompense et le nouvel état  \n",
    "          next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "          Q_table[state, action] += alpha * (reward + gamma * np.max(Q_table[next_state, :]) - Q_table[state, action])\n",
    "          \n",
    "          state = next_state\n",
    "     \n",
    "     epsilon *= epsilon_decay\n",
    "     \n",
    "env.close()\n",
    "print(Q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluation des Performances de l'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.00511548e-01 4.14119636e-01 3.76693284e-01 3.78210308e-01]\n",
      " [1.77531987e-03 2.46640496e-01 2.44252704e-05 4.70902233e-02]\n",
      " [2.97856491e-01 3.82499183e-03 2.98266617e-02 2.17263079e-03]\n",
      " [4.93375594e-05 2.01334495e-05 0.00000000e+00 0.00000000e+00]\n",
      " [5.19151020e-01 3.78449133e-01 3.09760910e-01 3.02645706e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.52072562e-02 4.58775896e-02 2.36401788e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.12541366e-01 3.47521604e-01 3.42891612e-01 5.48434568e-01]\n",
      " [3.30037817e-01 5.82593893e-01 4.00582797e-01 1.97671396e-01]\n",
      " [5.72796838e-01 8.32165307e-02 2.68754571e-01 5.05302320e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.17494349e-01 3.07277205e-01 6.98458190e-01 2.61115799e-01]\n",
      " [3.82545567e-01 8.93329212e-01 4.61369789e-01 4.71516230e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "Taux de réussite de l'agent sur 100 épisodes : 75.00%\n"
     ]
    }
   ],
   "source": [
    "# Évaluation de l'agent après entraînement\n",
    "num_test_episodes = 100\n",
    "successes = 0  # Nombre de fois où l'agent atteint l'objectif\n",
    "\n",
    "\n",
    "for episode in range(num_test_episodes):\n",
    "    state = env.reset()[0]  # Réinitialiser l'environnement\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Sélectionner l'action optimale (greedy policy)\n",
    "        action = np.argmax(Q_table[state])\n",
    "\n",
    "        # Exécuter l'action\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Si on atteint l'objectif (reward == 1), on compte une réussite\n",
    "        if reward == 1:\n",
    "            successes += 1\n",
    "\n",
    "        # Passer à l'état suivant\n",
    "        state = next_state\n",
    "        \n",
    "\n",
    "print(Q_table)\n",
    "\n",
    "# Calcul du taux de réussite\n",
    "taux_reussite = (successes / num_test_episodes) * 100\n",
    "print(f\"Taux de réussite de l'agent sur {num_test_episodes} épisodes : {taux_reussite:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
