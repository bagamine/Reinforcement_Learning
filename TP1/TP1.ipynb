{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>TP 1</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creation de l'environnement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.02836796, -0.04742816,  0.02288805,  0.02292759], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym \n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comprendre la structure d'un environnemnt Gym en explorant ses proprietes et ses actions possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(2)\n",
      "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action: 1, Observation: [-0.02931652  0.14735821  0.0233466  -0.26244694], Reward: 1.0\n",
      "Action: 0, Observation: [-0.02636936 -0.04808908  0.01809766  0.03750739], Reward: 1.0\n",
      "Action: 1, Observation: [-0.02733114  0.14676873  0.01884781 -0.24941105], Reward: 1.0\n",
      "Action: 1, Observation: [-0.02439576  0.3416165   0.01385959 -0.5360899 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.01756343  0.53654087  0.00313779 -0.8243738 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.00683261  0.7316198  -0.01334969 -1.1160681 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.00779978  0.9269144  -0.03567105 -1.4129087 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.02633807  1.1224598  -0.06392922 -1.7165253 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.04878727  0.9281266  -0.09825972 -1.4444033 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.0673498   0.73434156 -0.1271478  -1.1839697 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.08203663  0.9308628  -0.15082718 -1.5136528 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.10065389  0.73785424 -0.18110025 -1.271602  ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.11541097  0.5454457  -0.20653228 -1.040662  ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.12631989  0.35357592 -0.22734553 -0.81926364], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.03187763 -0.14572282 -0.01689324  0.30256283], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02896317 -0.34059998 -0.01084199  0.5898705 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02215117 -0.5355685   0.00095542  0.87911856], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.0114398  -0.73070335  0.01853779  1.1721017 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.00317426 -0.9260614   0.04197983  1.4705381 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.02169549 -1.121671    0.07139059  1.7760324 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.04412891 -0.92742217  0.10691123  1.5063725 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.06267735 -0.73374695  0.1370387   1.2488904 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.07735229 -0.54062164  0.1620165   1.0020818 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.08816472 -0.34799153  0.18205813  0.7643453 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.09512456 -0.5450912   0.19734503  1.1083385 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.10602638 -0.35303178  0.2195118   0.88348466], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.03206414 -0.23956299  0.04583217  0.32228246], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.02727287 -0.04512264  0.05227782  0.04439816], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02637042 -0.24095374  0.05316578  0.35310617], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.02155135 -0.4367898   0.0602279   0.6620686 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.01281555 -0.24255528  0.07346927  0.38894135], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.00796445 -0.43863896  0.0812481   0.70385396], Reward: 1.0\n",
      "Action: 0, Observation: [-8.0833310e-04 -6.3478726e-01  9.5325179e-02  1.0209665e+00], Reward: 1.0\n",
      "Action: 1, Observation: [-0.01350408 -0.44105574  0.11574451  0.75967175], Reward: 1.0\n",
      "Action: 1, Observation: [-0.02232519 -0.24770263  0.13093795  0.5055356 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.02727924 -0.4444031   0.14104865  0.8364465 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.03616731 -0.64114076  0.15777759  1.1699532 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.04899012 -0.8379228   0.18117665  1.5076543 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.06574858 -0.64540046  0.21132974  1.2765726 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01301627 -0.20655656  0.02649688  0.25297087], Reward: 1.0\n",
      "Action: 0, Observation: [-0.0171474  -0.40204662  0.03155629  0.55389225], Reward: 1.0\n",
      "Action: 0, Observation: [-0.02518833 -0.5975972   0.04263414  0.856348  ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.03714028 -0.7932733   0.0597611   1.1621262 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.05300574 -0.5989784   0.08300363  0.88876337], Reward: 1.0\n",
      "Action: 0, Observation: [-0.06498531 -0.79512274  0.10077889  1.206342  ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.08088776 -0.991392    0.12490573  1.528831  ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.10071561 -0.7979784   0.15548235  1.2775977 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.11667518 -0.6051423   0.1810343   1.0373607 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.12877803 -0.41282743  0.20178151  0.8065364 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.13703457 -0.22095811  0.21791224  0.58349365], Reward: 1.0\n",
      "Action: 1, Observation: [-0.02950495  0.15750507 -0.04923593 -0.2718078 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.02635484  0.35329375 -0.05467208 -0.57960445], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01928897  0.15897886 -0.06626417 -0.30463317], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01610939 -0.03513926 -0.07235684 -0.03356244], Reward: 1.0\n",
      "Action: 1, Observation: [-0.01681218  0.1609417  -0.07302809 -0.34816864], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01359334 -0.03306969 -0.07999146 -0.0793784 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01425474 -0.22695924 -0.08157903  0.18703353], Reward: 1.0\n",
      "Action: 0, Observation: [-0.01879392 -0.420825   -0.07783835  0.4529074 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.02721042 -0.61476487 -0.06878021  0.7200751 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.03950572 -0.8088712  -0.05437871  0.99034065], Reward: 1.0\n",
      "Action: 1, Observation: [-0.05568314 -0.61306524 -0.03457189  0.6810864 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.06794445 -0.80769044 -0.02095016  0.9626876 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.08409826 -1.0025247  -0.00169641  1.248716  ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.10414875 -0.80738103  0.02327791  0.95550215], Reward: 1.0\n",
      "Action: 1, Observation: [-0.12029637 -0.6125798   0.04238795  0.6702226 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.13254797 -0.41807202  0.0557924   0.39118126], Reward: 1.0\n",
      "Action: 0, Observation: [-0.1409094  -0.6139396   0.06361603  0.7009199 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.1531882  -0.809883    0.07763442  1.012931  ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.16938587 -1.00595     0.09789304  1.3289472 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.18950486 -0.81219     0.12447199  1.0684334 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.20574866 -1.0087187   0.14584066  1.3974462 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.22592303 -1.205322    0.17378958  1.7319458 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.25002947 -1.4019502   0.20842849  2.0732825 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.27806848 -1.2094669   0.24989414  1.8516417 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.00328061 -0.21690843 -0.02162393  0.31514195], Reward: 1.0\n",
      "Action: 0, Observation: [-0.00105756 -0.41171578 -0.01532109  0.6009277 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.00929187 -0.6066201  -0.00330253  0.88874567], Reward: 1.0\n",
      "Action: 1, Observation: [-0.02142427 -0.4114535   0.01447238  0.5950264 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.02965334 -0.21653704  0.02637291  0.30693707], Reward: 1.0\n",
      "Action: 0, Observation: [-0.03398408 -0.41202468  0.03251165  0.60781926], Reward: 1.0\n",
      "Action: 0, Observation: [-0.04222458 -0.6075857   0.04466803  0.9105626 ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.05437629 -0.4130958   0.06287929  0.6322467 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.06263821 -0.609036    0.07552422  0.94405055], Reward: 1.0\n",
      "Action: 1, Observation: [-0.07481893 -0.4150083   0.09440523  0.67602164], Reward: 1.0\n",
      "Action: 1, Observation: [-0.08311909 -0.22131614  0.10792566  0.41449103], Reward: 1.0\n",
      "Action: 1, Observation: [-0.08754542 -0.02787619  0.11621548  0.15768793], Reward: 1.0\n",
      "Action: 0, Observation: [-0.08810294 -0.22445367  0.11936924  0.48465565], Reward: 1.0\n",
      "Action: 1, Observation: [-0.09259202 -0.03120067  0.12906235  0.23185022], Reward: 1.0\n",
      "Action: 0, Observation: [-0.09321603 -0.22790791  0.13369936  0.56229615], Reward: 1.0\n",
      "Action: 0, Observation: [-0.09777419 -0.42462763  0.14494528  0.89393294], Reward: 1.0\n",
      "Action: 0, Observation: [-0.10626674 -0.62138635  0.16282395  1.228441  ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.11869447 -0.42869034  0.18739276  0.990879  ], Reward: 1.0\n",
      "Action: 1, Observation: [-0.12726827 -0.23650302  0.20721035  0.7624165 ], Reward: 1.0\n",
      "Action: 0, Observation: [-0.13199833 -0.4337847   0.22245868  1.1124936 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.02841124  0.20304827 -0.02890212 -0.29263434], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.03247221  0.39857012 -0.03475481 -0.5942907 ], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.04044361  0.59416085 -0.04664062 -0.89771545], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.05232683  0.3997011  -0.06459493 -0.6200504 ], Reward: 1.0\n",
      "Action: 0, Observation: [ 0.06032085  0.20553795 -0.07699594 -0.34839076], Reward: 1.0\n",
      "Action: 1, Observation: [ 0.06443161  0.40166575 -0.08396375 -0.6643253 ], Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "\n",
    "for _ in range(100):\n",
    "    action=env.action_space.sample()\n",
    "    obs, reward, done, _,_ = env.step(action)\n",
    "    print(f\"Action: {action}, Observation: {obs}, Reward: {reward}\")\n",
    "    if done:\n",
    "        env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exercice2: Manipulation des Observation et Recompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0\n",
      "Observation: [ 0.03535121 -0.16262314 -0.03101585  0.23644595]\n",
      "Récompense: 1.0\n",
      "Action: 1, Observation: [ 0.03209874  0.03292789 -0.02628694 -0.06585672], Récompense: 1.0\n",
      "Action: 1, Observation: [ 0.0327573   0.22841667 -0.02760407 -0.36671606], Récompense: 1.0\n",
      "Action: 0, Observation: [ 0.03732563  0.03369763 -0.03493839 -0.08286333], Récompense: 1.0\n",
      "Action: 0, Observation: [ 0.03799959 -0.16090652 -0.03659566  0.19859496], Récompense: 1.0\n",
      "Action: 1, Observation: [ 0.03478146  0.03471923 -0.03262376 -0.10540392], Récompense: 1.0\n",
      "Action: 1, Observation: [ 0.03547584  0.23029314 -0.03473184 -0.4081984 ], Récompense: 1.0\n",
      "Action: 1, Observation: [ 0.0400817   0.4258899  -0.04289581 -0.7116257 ], Récompense: 1.0\n",
      "Action: 0, Observation: [ 0.0485995   0.23138736 -0.05712832 -0.43274775], Récompense: 1.0\n",
      "Action: 0, Observation: [ 0.05322725  0.03711885 -0.06578327 -0.15860695], Récompense: 1.0\n",
      "Action: 1, Observation: [ 0.05396963  0.23311792 -0.06895541 -0.47129598], Récompense: 1.0\n",
      "Action: 0, Observation: [ 0.05863198  0.03903425 -0.07838133 -0.20111917], Récompense: 1.0\n",
      "Action: 1, Observation: [ 0.05941267  0.23518461 -0.08240371 -0.5174619 ], Récompense: 1.0\n",
      "Action: 1, Observation: [ 0.06411636  0.43136427 -0.09275295 -0.8349321 ], Récompense: 1.0\n",
      "Action: 1, Observation: [ 0.07274365  0.6276226  -0.10945159 -1.1552845 ], Récompense: 1.0\n",
      "Action: 1, Observation: [ 0.08529609  0.8239881  -0.13255729 -1.4801848 ], Récompense: 1.0\n",
      "Action: 1, Observation: [ 0.10177586  1.0204552  -0.16216098 -1.811158  ], Récompense: 1.0\n",
      "Action: 0, Observation: [ 0.12218496  0.82746977 -0.19838414 -1.5729442 ], Récompense: 1.0\n",
      "Action: 0, Observation: [ 0.13873436  0.63518965 -0.22984302 -1.348124  ], Récompense: 1.0\n",
      "Action: 1, Observation: [ 0.01095842  0.19927827  0.03386553 -0.27931872], Récompense: 1.0\n",
      "Action: 0, Observation: [0.01494398 0.00369    0.02827916 0.02385004], Récompense: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Exécution d'une action et affichage des résultats\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation, _ = env.reset()\n",
    "action = env.action_space.sample()  # Choisir une action aléatoire\n",
    "observation, reward, done, _, _ = env.step(action)\n",
    "\n",
    "print(f\"Action: {action}\")\n",
    "print(f\"Observation: {observation}\")\n",
    "print(f\"Récompense: {reward}\")\n",
    "# Boucle de jeu\n",
    "for _ in range(20):\n",
    "    action = env.action_space.sample()  # Choisir une action aléatoire\n",
    "    observation, reward, done, _, _ = env.step(action)\n",
    "    print(f\"Action: {action}, Observation: {observation}, Récompense: {reward}\")\n",
    "    if done:\n",
    "        observation, _ = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exercice 3 : Contrôle Manuel de l’Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action invalide, veuillez entrer 0 ou 1.\n",
      "Observation: [ 0.00098242  0.18773976 -0.04881332 -0.29518607], Récompense: 1.0\n",
      "Observation: [ 0.00473721  0.3835224  -0.05471705 -0.6028556 ], Récompense: 1.0\n",
      "Observation: [ 0.01240766  0.57936525 -0.06677416 -0.91225904], Récompense: 1.0\n",
      "Observation: [ 0.02399497  0.775324   -0.08501934 -1.2251592 ], Récompense: 1.0\n",
      "Observation: [ 0.03950144  0.5813934  -0.10952252 -0.9602796 ], Récompense: 1.0\n",
      "Observation: [ 0.05112932  0.3879005  -0.12872812 -0.70391446], Récompense: 1.0\n",
      "Observation: [ 0.05888733  0.19477518 -0.14280641 -0.45436502], Récompense: 1.0\n",
      "Observation: [ 0.06278282  0.3915971  -0.1518937  -0.78843504], Récompense: 1.0\n",
      "Observation: [ 0.07061477  0.5884425  -0.16766241 -1.1247889 ], Récompense: 1.0\n",
      "Observation: [ 0.08238362  0.78531706 -0.19015819 -1.4650217 ], Récompense: 1.0\n",
      "Observation: [ 0.09808996  0.98219043 -0.21945861 -1.8105788 ], Récompense: 1.0\n",
      "Durée totale de l'épisode : 11 étapes\n"
     ]
    }
   ],
   "source": [
    "observation, _ = env.reset() # renitialise et recupere etat init de env\n",
    "done = False\n",
    "steps = 0\n",
    "\n",
    "while not done:\n",
    "    action = int(input(\"Entrez une action (0 ou 1) : \"))  \n",
    "    if action not in [0, 1]:\n",
    "        print(\"Action invalide, veuillez entrer 0 ou 1.\")\n",
    "        continue\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action) # execute action et recupere\n",
    "    print(f\"Observation: {observation}, Récompense: {reward}\") #observation et reward apres action\n",
    "\n",
    "    steps += 1\n",
    "    done = terminated or truncated  # truncated limite temps\n",
    "\n",
    "print(f\"Durée totale de l'épisode : {steps} étapes\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exercice 4 : Évaluation des Performances d’une Politique Aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épisode 1: Durée = 14\n",
      "Épisode 2: Durée = 12\n",
      "Épisode 3: Durée = 18\n",
      "Épisode 4: Durée = 28\n",
      "Épisode 5: Durée = 14\n",
      "Épisode 6: Durée = 18\n",
      "Épisode 7: Durée = 28\n",
      "Épisode 8: Durée = 12\n",
      "Épisode 9: Durée = 13\n",
      "Épisode 10: Durée = 18\n",
      "\n",
      "Durée moyenne sur 10 épisodes: 17.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_episodes = 10  # Nombre d'épisodes à exécuter\n",
    "durations = []  # Liste pour stocker la durée de chaque épisode\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    timestep = 0  \n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()  \n",
    "        _, _, done, _, _ = env.step(action)\n",
    "        timestep += 1  # Incrémenter la durée de l'épisode\n",
    "    \n",
    "    durations.append(timestep)  \n",
    "    print(f\"Épisode {episode + 1}: Durée = {timestep}\")\n",
    "\n",
    "# Calcul et affichage de la durée moyenne\n",
    "average_duration = np.mean(durations)\n",
    "print(f\"\\nDurée moyenne sur {num_episodes} épisodes: {average_duration}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
